# 腾讯微博备份工具
这是一个基于Python3的腾讯微博爬虫，将数据保存在db中  
此项目思路源于项目 tencent-weibo-exporter ，感谢原作者，欢迎issues  
思路简单粗暴，爬取网页版的内容……  
特别弱鸡，求大佬们不喷  
  
配置环境：
-
需要selenium来对Firefox进行控制(请安装Firefox)  
pip install selenium  
下载geckodriver，将geckodriver.exe放入Python3根目录下  
由于依靠QQ自动登陆登陆微博所以需要Windows，后续版本可能会添加对非登陆方式的支持  
（但还是建议使用登陆方式，因为一页显示的内容更多  
  
特性  
-
数据库存储，方便查找  
支持多种类型的微博，支持多图，成图，表情，视频等多种类型  
先爬取内容存入数据库，在后期进行下载图片  
支持“断点续传”，可以找回上次未完成的备份继续进行  
自动解决只能翻页一百页的问题，爬取所有内容  
效率大概一小时翻300页，每页固定30条，也就是9000条/小时
  
数据库结构说明  
-
EMOJI表：存储EMOJI名字（如果有）和链接  
VIDEO表：存储视频名，对应微博CID，原链接，封面  
IMAGE表：存储MINE页（广播/评论）配图和AT页配图，用cid索引一条微博里的图片，rank表示顺序，转评插入的图片仅有链接（因为可以从正文标签过来查找  
INFO表：暂时仅用来存储上次的进度  
MINE表：存储MINE页原文信息和位置信息，emoji在原文中用<emoji>标签标识链接，转评配图用<img>标识链接  
	type列含义：1 原创；2 转播；3 评论；12 赞 
AT表：存储AT页原文信息和位置信息
FAVOR表：同MINE，用于存储favor页（收藏）内容  
FIMAGE表：同IMAGE，用于存储favor页（收藏）配图  
PMT表：主要用于记录有私信记录的人的ID

更新日志  
-
v5
新增私信抓取功能，但是对于量比较大的，自动加载更多有一定等待时间，千万不要手动去点加载更多，否则会搞乱程序的判断导致翻不到底，其次也做了人工判断是否翻到了末尾，给手动翻页的机会
私信比较简单，表情图片文字，没了，应该不会有漏的东西吧…………

v4  
新增抓取提及的功能，与抓取其他相互独立互不影响，可以单独抓取（提及实在太多了，小心火葬场  
（由于提及与MINE应该有大量图片重复，所以共用IMAGE表和mine_image文件夹  
彻底解决了弹出登陆窗口的问题  
用新算法处理，速度提高二十倍，速度极快，所以现在的话对图片库进行增量更新不再痛苦了  
（建议运行两次download_pic，以免有图片因为网络问题导致没下载成功  
新增图片投票功能（很抱歉，之前没有考虑到这个问题，可以直接增量备份，但是约等于重来了…………

v3.5  
新增抓取收藏的功能，与抓取广播相互独立互不影响  

v3  
增加图片下载功能，会区分类型分为不同文件夹  
（视频图片404不必担心，很多确实已经凉了，包括腾讯微博来源的久远的url.cn短链接也被清理了不能转跳  
添加日志功能  

v2.1  
本人实测速度大概160页/小时，V2版本只有两条原文中有错误取了纯文本，速度凑合，但是非常稳定，可以一次跑完没有错误中断了  
基本不会动了，下版主要增加图片下载  

增加了一些能力处理引号的能力（目前仅能处理单一种类的引号，实际上也基本够用了）  
由于以上，插入失败的url会自动打印并导出供手工处理（起码让你知道那些没有成功插入）  
修复了一百页无法跳转的问题（重要）  
修复了页面有时没有正常跳转到下一页导致计数错误的问题  
修复对用户名内emoji的报错（其实就是直接不管了取纯文本（  